{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cstraehl/anaconda3/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.tri as tri\n",
    "from scipy.special import betaln\n",
    "from scipy.stats import beta\n",
    "from scipy import stats\n",
    "from scipy.special import gamma as gamma_fun\n",
    "import scipy.special as spec\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from deep_bayes.models import BayesFlow, SequenceNetwork\n",
    "from deep_bayes.training import train_online\n",
    "from deep_bayes.losses import maximum_likelihood_loss\n",
    "from deep_bayes.viz import plot_true_est_scatter, plot_true_est_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CPU as available physical device\n",
    "#my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "#tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward model priors and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(batch_size):\n",
    "    \"\"\"\n",
    "    Samples from the prior 'batch_size' times.\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int -- the number of samples to draw from the prior\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    theta : np.ndarray of shape (batch_size, theta_dim) -- the samples batch of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = np.random.uniform(low=[0.35, 0.1, 0.1, 0.1, 0.7], \n",
    "                              high=[2.25, 0.9, 0.9, 0.9, 1.0], size=(batch_size, 5))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(params, t, init_values):\n",
    "    \"\"\"\n",
    "    Runs the forward model ones, i.e., generates a sample from p(x|theta).\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    params : np.ndarray of shape (theta_dim, ) -- the data generating parameters\n",
    "    n_obs  : int -- the numebr of observations to draw from p(x|theta)\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    x      : np.ndarray of shape (n_obs, x_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    S_0, E_0, I_0, A_0, R_0 = init_vals\n",
    "    S, E, I, A, R = [S_0], [E_0], [I_0], [A_0], [R_0]\n",
    "    \n",
    "    #extract time dependent alphas from params\n",
    "    alphas = params[0:6]\n",
    "    alphas_t = params[6:12]\n",
    "    alphas_t_asi = np.argsort(alphas_t)\n",
    "    alphas = alphas[alphas_t_asi]\n",
    "    alphas_t = alphas_t[alphas_t_asi].astype(np.int32)\n",
    "    \n",
    "    #construct dense alphas array\n",
    "    alphas_dense = np.zeros((t.shape[0]))\n",
    "    for i in range(alphas_t.shape[0]):\n",
    "        alphas_dense[alphas_t[i]:] = alphas[i]\n",
    "    \n",
    "    beta, gamma, theta, gamma_A, rho = params[12:]\n",
    "    \n",
    "    dt = t[1] - t[0]\n",
    "    for i,_ in enumerate(t[1:]):\n",
    "        next_S = S[-1] - (beta*S[-1]*(I[-1] + theta*A[-1]))*dt\n",
    "        next_E = E[-1] + (beta*S[-1]*(I[-1] + theta*A[-1]) - alphas_dense[i]*E[-1])*dt\n",
    "        next_I = I[-1] + (rho*alphas_dense[i]*E[-1] - gamma*I[-1])*dt\n",
    "        next_A = A[-1] + ((1 - rho)*alphas_dense[i]*E[-1] - gamma_A*A[-1])*dt\n",
    "        next_R = R[-1] + (gamma*I[-1] + gamma_A*A[-1])*dt\n",
    "        S.append(next_S)\n",
    "        E.append(next_E)\n",
    "        I.append(next_I)\n",
    "        A.append(next_A)\n",
    "        R.append(next_R)\n",
    "        \n",
    "    return np.stack([alphas_dense, S, E, I, A, R]).T\n",
    "\n",
    "\n",
    "t_max = 100\n",
    "dt = 1\n",
    "t = np.linspace(0, t_max, int(t_max/dt) + 1)\n",
    "N = 10000\n",
    "init_vals = 1 - 1/N, 1/N, 0, 0, 0\n",
    "forward_model = partial(forward_model, t=t, init_values=init_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, to_tensor=True, **args):\n",
    "    \"\"\"\n",
    "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
    "    theta ~ p(theta) and running x ~ p(x|theta).\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int -- the number of samples to draw from the prior\n",
    "    to_tensor  : boolean -- converts theta and x to tensors if True\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    theta : tf.Tensor or np.ndarray of shape (batch_size, theta_dim) - the data gen parameters \n",
    "    x     : tf.Tensor of np.ndarray of shape (batch_size, n_obs, x_dim)  - the generated data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample from prior\n",
    "    # theta is a np.array of shape (batch_size, theta_dim)\n",
    "    theta = prior(batch_size)\n",
    "    \n",
    "    #generate 3x 0.9 alpha at timestep 0\n",
    "    alphas0 = np.ones((batch_size,3))*0.9\n",
    "    alphas_t0 = np.zeros((batch_size,3))\n",
    "    \n",
    "    #generate 3 random alphas between 0.1 and 0.9\n",
    "    alphas = np.random.rand(batch_size,3)*0.8+0.1\n",
    "    n_obs = t_max\n",
    "    alphas_t = np.random.randint(n_obs, size=(batch_size,3))\n",
    "    \n",
    "    # construct alphaparam matrix\n",
    "    alphas = np.concatenate([alphas0, alphas],axis=1)\n",
    "    alphas_t = np.concatenate([alphas_t0, alphas_t],axis=1)\n",
    "    alphaparams = np.concatenate([alphas,alphas_t],axis=1)\n",
    "    \n",
    "    #construct new theta\n",
    "    thetap = np.concatenate([alphaparams, theta],axis=1)\n",
    "    \n",
    "    # Generate data\n",
    "    # x is a np.ndarray of shape (batch_size, n_obs, x_dim)\n",
    "    x = np.apply_along_axis(forward_model, axis=1, arr=thetap, **args)\n",
    "    \n",
    "    # Convert to tensor, if specified \n",
    "    if to_tensor:\n",
    "        theta = tf.convert_to_tensor(theta, dtype=tf.float32)\n",
    "        alphaparams = tf.convert_to_tensor(alphaparams, dtype=tf.float32)\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    return {'theta': theta, 'alphaparams':alphaparams, 'x': x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "inv_meta = {\n",
    "    'n_units': [128, 128, 128],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.00001,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "n_inv_blocks = 6\n",
    "\n",
    "\n",
    "summary_meta = {\n",
    "    'lstm_units' :  64,\n",
    "    'conv_meta'  : [\n",
    "            dict(filters=64, kernel_size=5, strides=1, activation='elu', kernel_initializer='glorot_normal', padding='same'),\n",
    "            dict(filters=64, kernel_size=3, strides=1, activation='elu', kernel_initializer='glorot_normal', padding='same'),\n",
    "            dict(filters=64, kernel_size=3, strides=1, activation='elu', kernel_initializer='glorot_normal', padding='same'),\n",
    "            dict(filters=64, kernel_size=3, strides=1, activation='elu', kernel_initializer='glorot_normal', padding='same'),\n",
    "            dict(filters=64, kernel_size=3, strides=1, activation='elu', kernel_initializer='glorot_normal', padding='same'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Forward model hyperparameters\n",
    "param_names = [r'$\\beta$', r'$\\gamma$', r'$\\theta$', r'$\\gamma_A$', r'$\\rho$']\n",
    "theta_dim = len(param_names)\n",
    "n_test = 500\n",
    "\n",
    "\n",
    "# Training and optimizer hyperparameters\n",
    "ckpt_file = \"SEIAR\"\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "iterations_per_epoch = 1000\n",
    "n_samples_posterior = 2000\n",
    "clip_value = 5.\n",
    "\n",
    "starter_learning_rate = 0.001\n",
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "decay_steps = 1000\n",
    "decay_rate = .95\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                           decay_steps, decay_rate, staircase=True)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_data = data_generator(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = SequenceNetwork(summary_meta)\n",
    "model = BayesFlow(inv_meta, n_inv_blocks, theta_dim, summary_net=summary_net, permute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile \n",
    "<p>In other words, run and plot performance of untrained networks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_est_scatter(model, test_data['x'], test_data['theta'], \n",
    "                      n_samples_posterior, param_names, figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/{}'.format(ckpt_file), max_to_keep=3)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ep in range(1, epochs+1):\n",
    "    with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        losses = train_online(model=model, \n",
    "                              optimizer=optimizer, \n",
    "                              data_gen=data_generator, \n",
    "                              loss_fun=maximum_likelihood_loss, \n",
    "                              iterations=iterations_per_epoch,\n",
    "                              batch_size=batch_size,\n",
    "                              p_bar=p_bar,\n",
    "                              clip_value=clip_value,\n",
    "                              clip_method='value',\n",
    "                              global_step=global_step)\n",
    "        \n",
    "        plot_true_est_scatter(model, test_data['x'], test_data['theta'], \n",
    "                      n_samples_posterior, param_names, figsize=(8, 5))\n",
    "        \n",
    "        plot_true_est_posterior(model, 2000, param_names, font_size=8,\n",
    "                        X_test=test_data['x'][:5], \n",
    "                        theta_test=test_data['theta'][:5], figsize=(8, 6))\n",
    "\n",
    "        # Manage checkpoint\n",
    "        manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_est_scatter(model, test_data['x'], test_data['theta'], \n",
    "                      n_samples_posterior, param_names, figsize=(12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
